{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaTokenizer,\n",
    "    HfArgumentParser,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from trl import (\n",
    "    AutoModelForCausalLMWithValueHead,\n",
    "    PPOConfig,\n",
    "    PPOTrainer,\n",
    "    create_reference_model,\n",
    "    set_seed,\n",
    ")\n",
    "from trl.core import LengthSampler\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PPOConfig.__init__() got an unexpected keyword argument 'model_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m script_args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args_into_dataclasses()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Configure PPO training parameters\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m ppo_config \u001b[38;5;241m=\u001b[39m \u001b[43mPPOConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscript_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscript_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_with\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscript_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_with\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mppo_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscript_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mppo_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscript_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmini_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscript_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscript_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: PPOConfig.__init__() got an unexpected keyword argument 'model_name'"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. Define Script Arguments\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    GPT-J + LoRA + PPO for detox (or other) fine-tuning\n",
    "    \"\"\"\n",
    "    model_name: Optional[str] = field(\n",
    "        default=\"EleutherAI/gpt-j-6B\",\n",
    "        metadata={\"help\": \"The base model name or path, e.g., EleutherAI/gpt-j-6B\"}\n",
    "    )\n",
    "    log_with: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Use 'wandb' or None for logging\"}\n",
    "    )\n",
    "    learning_rate: Optional[float] = field(\n",
    "        default=1e-5,\n",
    "        metadata={\"help\": \"Learning rate for PPO/LoRA training\"}\n",
    "    )\n",
    "    mini_batch_size: Optional[int] = field(\n",
    "        default=2,\n",
    "        metadata={\"help\": \"Minibatch size for PPO updates\"}\n",
    "    )\n",
    "    batch_size: Optional[int] = field(\n",
    "        default=8,\n",
    "        metadata={\"help\": \"Batch size for sampling in PPOTrainer\"}\n",
    "    )\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"Number of gradient accumulation steps\"}\n",
    "    )\n",
    "    ppo_epochs: Optional[int] = field(\n",
    "        default=3,\n",
    "        metadata={\"help\": \"Number of PPO training epochs\"}\n",
    "    )\n",
    "    model_save_path: Optional[str] = field(\n",
    "        default=\"./gptj-lora-ppo-detox\",\n",
    "        metadata={\"help\": \"Directory to save the final model\"}\n",
    "    )\n",
    "    seed: Optional[int] = field(\n",
    "        default=42,\n",
    "        metadata={\"help\": \"Random seed for reproducibility\"}\n",
    "    )\n",
    "\n",
    "# Parse the script arguments\n",
    "parser = HfArgumentParser(ScriptArguments)\n",
    "script_args = parser.parse_args_into_dataclasses()[0]\n",
    "\n",
    "# Configure PPO training parameters\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=script_args.model_name,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    log_with=script_args.log_with,\n",
    "    ppo_epochs=script_args.ppo_epochs,\n",
    "    mini_batch_size=script_args.mini_batch_size,\n",
    "    batch_size=script_args.batch_size,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. Build/Load Dataset\n",
    "#    Using allenai/real-toxicity-prompts\n",
    "# -----------------------------\n",
    "def build_dataset(\n",
    "    config,\n",
    "    dataset_name=\"allenai/real-toxicity-prompts\",\n",
    "    input_min_text_length=5,\n",
    "    input_max_text_length=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Load and preprocess the real-toxicity-prompts dataset.\n",
    "    Filters samples with toxicity > 0.3 and truncates to [5, 10] token lengths.\n",
    "    You can customize this function to use a different dataset or preprocessing logic.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    # GPT-J likely already has an eos_token; set pad_token to eos_token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load the dataset\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "    # Filter out samples with toxicity <= 0.3 or missing toxicity scores\n",
    "    def filter_fn(sample):\n",
    "        toxicity = sample[\"prompt\"][\"toxicity\"]\n",
    "        return (toxicity is not None) and (toxicity > 0.3)\n",
    "\n",
    "    ds = ds.filter(filter_fn, batched=False)\n",
    "\n",
    "    # Sample input sizes between min and max lengths\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    # Tokenize and truncate the samples\n",
    "    def tokenize(sample):\n",
    "        prompt = sample[\"prompt\"][\"text\"]\n",
    "        continuation = sample[\"continuation\"][\"text\"]\n",
    "        tokens = tokenizer.encode(prompt + continuation)\n",
    "        tokens = tokens[: input_size()]  # Truncate\n",
    "        sample[\"input_ids\"] = tokens\n",
    "        sample[\"query\"] = tokenizer.decode(tokens)\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "\n",
    "    # Split the dataset into training and testing; here we take the training portion\n",
    "    ds = ds.train_test_split(test_size=0.1, shuffle=True)[\"train\"]\n",
    "    return ds\n",
    "\n",
    "# Collate function for PPOTrainer\n",
    "def collator(data):\n",
    "    return {key: [d[key] for d in data] for key in data[0]}\n",
    "\n",
    "# We retrieve the dataloader by calling the `build_dataset` function.\n",
    "min_input_length = 30\n",
    "max_input_length = 40\n",
    "dataset = build_dataset(ppo_config, input_min_text_length=min_input_length, input_max_text_length=max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Set Random Seed\n",
    "# -----------------------------\n",
    "set_seed(script_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Load GPT-J Base Model + Apply LoRA\n",
    "# -----------------------------\n",
    "# Load the base GPT-J model with float16 precision to save memory\n",
    "print(\">>> Loading GPT-J base model:\", script_args.model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Apply LoRA to the base model\n",
    "print(\">>> Applying LoRA ...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # GPT-J's attention projection layer names\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    fan_in_fan_out=False,\n",
    ")\n",
    "base_model = get_peft_model(base_model, lora_config)\n",
    "base_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Convert to Model with Value Head\n",
    "# -----------------------------\n",
    "print(\">>> Converting to AutoModelForCausalLMWithValueHead ...\")\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. Create Reference Model & Optimizer\n",
    "# -----------------------------\n",
    "# PPO requires a reference model to compute KL divergence for policy updates\n",
    "# Here, we do not share any layers (num_shared_layers=0) to keep models independent\n",
    "ref_model = create_reference_model(model, num_shared_layers=0)\n",
    "\n",
    "# Initialize the optimizer with parameters that require gradients\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=ppo_config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7. Initialize Tokenizer and PPOTrainer\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    ppo_config,\n",
    "    model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    data_collator=collator,\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 8. Prepare Reward Model: Roberta Hate-Speech\n",
    "#    (facebook/roberta-hate-speech-dynabench-r4-target)\n",
    "#    You can replace this with your own reward model, e.g., for toxicity/sentiment\n",
    "# -----------------------------\n",
    "toxicity_model_id = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = RobertaTokenizer.from_pretrained(toxicity_model_id)\n",
    "toxicity_model = RobertaForSequenceClassification.from_pretrained(\n",
    "    toxicity_model_id,\n",
    "    torch_dtype=torch.float16\n",
    ").to(ppo_trainer.accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 9. Define Generation Parameters & Training Loop\n",
    "# -----------------------------\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "# Sample output lengths between 5 and 15 tokens\n",
    "output_length_sampler = LengthSampler(5, 15)\n",
    "model_save_path = script_args.model_save_path\n",
    "\n",
    "print(\">>> Starting PPO training ...\")\n",
    "\n",
    "for step, batch in tqdm(enumerate(ppo_trainer.dataloader), total=len(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # 1) Generate responses using the policy model\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "\n",
    "        # ppo_trainer.generate() wraps model.generate()\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        # Take only the last gen_len tokens as the response\n",
    "        response_tensors.append(response.squeeze()[-gen_len:])\n",
    "\n",
    "    # Decode responses to text\n",
    "    batch[\"response\"] = [tokenizer.decode(r) for r in response_tensors]\n",
    "\n",
    "    # 2) Compute rewards using the reward model\n",
    "    texts = batch[\"response\"]\n",
    "    toxicity_inputs = toxicity_tokenizer(\n",
    "        texts, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    ).to(ppo_trainer.accelerator.device)\n",
    "\n",
    "    logits = toxicity_model(**toxicity_inputs).logits.float()\n",
    "    # Assuming logits[:, 0] represents the hate-speech/toxicity score\n",
    "    # In practice, you might need to apply sigmoid or softmax and map scores appropriately\n",
    "    toxicity_labels = (logits[:, 0]).tolist()\n",
    "    rewards = [torch.tensor(score) for score in toxicity_labels]\n",
    "\n",
    "    # 3) Perform a PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    # 4) Save the model checkpoint periodically\n",
    "    if step % 100 == 0:\n",
    "        if ppo_trainer.accelerator.is_main_process:\n",
    "            print(f\">>> Saving model checkpoint at step {step} ...\")\n",
    "            ppo_trainer.save_pretrained(model_save_path)\n",
    "\n",
    "print(\">>> PPO training done. Saving final model ...\")\n",
    "if ppo_trainer.accelerator.is_main_process:\n",
    "    ppo_trainer.save_pretrained(model_save_path)\n",
    "\n",
    "print(\"All finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
